{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":[],"mount_file_id":"1mI75kqF-n5U1UU9OqDFN7BIRv22xGbeV","authorship_tag":"ABX9TyMIMv9ONkathuaNpEnsQdby"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"metadata":{"id":"VX1mtwcoXfYJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn import model_selection, naive_bayes,svm\n","from nltk.corpus import wordnet as wn\n","from collections import defaultdict\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.utils.fixes import loguniform\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","# Load training data\n","# sentences=pd.read_csv('/content/gdrive/MyDrive/NCKH/Data/Question_label.csv')\n","with open(\"/content/gdrive/MyDrive/NCKH/Data/Question_label.csv\", encoding='utf-8') as f:\n","    sentences = f.read().split(\"\\n\")\n","\n","label = []\n","interaction = []\n","\n","for sentence in sentences[:-1]:\n","    sentence = sentence.split(\";\")\n","    label.append(sentence[0])\n","    interaction.append(sentence[1])\n","\n","\n","\n","# Load stop words\n","with open(\"/content/gdrive/MyDrive/NCKH/Data/stop_word.txt\", encoding=\"utf-8\") as f:\n","    pt_stop_words = f.read().split(\"\\n\")\n","\n","# Features\n","\n","tfidf = TfidfVectorizer(sublinear_tf=True,\n","                        min_df=5, norm='l2',\n","                        encoding='latin-1',\n","                        ngram_range=(1, 2),\n","                        stop_words=pt_stop_words)\n","features = tfidf.fit_transform(interaction).toarray()\n","labels = label\n","\n","count_vect = CountVectorizer()\n","X_train_counts = count_vect.fit_transform(interaction)\n","tfidf_transformer = TfidfTransformer()\n","X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n","\n","# SVM Model\n","svm_model = svm.SVC(kernel='linear').fit(X_train_tfidf, labels)\n","# print(\"SVM Accuracy Score ->>>>\",accuracy_score(prediction_SVM,y_test)*100)\n","\n","def classification(interaction):\n","    return svm_model.predict(count_vect.transform([interaction]))[0]"],"metadata":{"id":"UNQFIqMC9uHD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","import csv\n","import random\n","import numpy\n","import numpy as np\n","from sklearn.svm import SVC\n","from numpy.linalg import norm\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","question = input(\"Xin chào,mời hập câu hỏi:\")\n","# def load_answers(path):\n","#     answers = dict()\n","#     with open(path) as csvfile:\n","#         readCSV = csv.reader(csvfile, delimiter=\";\")\n","#         for row in readCSV:\n","#             answers[row[0]] = row[1:]\n","#     return answers\n","\n","\n","def reply(intention):\n","  def cosine(x, y):\n","    cos_sim = np.dot(x, y)/(norm(x)*norm(y))\n","    return cos_sim\n","\n","  def arrx(arr,n, x):\n","    a = []\n","    for i in range(n):\n","        a.append(arr[x, i])\n","    return a\n","  \n","  rd=pd.read_csv('/content/gdrive/MyDrive/NCKH/Data/Intent-Question-Answer.csv',encoding=\"utf-8\")\n","  rd.head()\n","  rd_intent_unique=[intention]\n","  rd_sv=rd[rd.Intent.isin(rd_intent_unique)]\n","  rd_sv.to_csv('/content/gdrive/MyDrive/NCKH/Data/temporary_save.csv',encoding=\"utf-8\",index=False)\n","  data = []\n","  answer=[]\n","  with open('/content/gdrive/MyDrive/NCKH/Data/temporary_save.csv', newline='',encoding=\"utf-8\" ) as csvfile:\n","    reader = csv.DictReader(csvfile)\n","    for row in reader:\n","        answer.append(row[\"Answer\"])\n","        data.append(row['Question'])\n","  test1 =question\n","  test =[]\n","  test.append(test1)\n","\n","\n","# khởi tạo\n","  counter = CountVectorizer(min_df=1)\n","# tạo data bằng counter\n","  tf_matrix = counter.fit_transform(data)\n","  arr = tf_matrix.todense()\n","\n","# khởi tạo\n","  v = TfidfVectorizer()\n","# tạo data bằng vectorizen\n","  vectors_train = v.fit_transform(data)\n","  arrv =vectors_train.todense()\n","\n","# vector hóa dữ liệu test\n","  vt =counter.transform(test).toarray()\n","  vy= arrx(vt,len(counter.vocabulary_) ,0)#chuyển lại mãng 1 chiều\n","\n","  answercorrect =0\n","  correct =0.5\n","  for i in range(0, len(answer)):\n","    vx = arrx(arrv,len(counter.vocabulary_) ,i)#chuyển 1 chiều của vector công việc thứ i\n","    # print(cosine(vy, vx))\n","    if cosine(vy,vx) >correct:\n","        correct =cosine(vy,vx)\n","        answercorrect=i\n","  print(correct)\n","  print(answer[answercorrect])\n","def main():\n","    # path = \"/content/drive/MyDrive/NCKH/Data/Answer_label.csv\"\n","    # answers = load_answers(path)\n","    \n","\n","    intention = \"\"\n","\n","   \n","    # question = input()\n","    # while intention != \"Bye\":\n","    #   question = input()\n","    #   if question in [\"Hẹn gặp lại\", \"Gặp lại sau\", \"Tạm biệt\", \"Bye\", \"Bái bai\"]:\n","    #         print(\"OK, hẹn gặp lại!\")\n","    #         break\n","    #   if question in ['Xin chào', 'Hi','Hello']:\n","    #         print(\"Xin chào, bạn cần tôi giúp điều gì ?\")\n","    # else:\n","    intention = classification(question)\n","    reply(intention)\n","# text=intention\n","\n","main()"],"metadata":{"id":"UHymqzvF-if0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VFffnUi05zY7"},"execution_count":null,"outputs":[]}]}